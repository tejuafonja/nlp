{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_with_torchtext.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejuafonja/nlp/blob/master/text_classification_with_torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_q7tE3vW--I",
        "colab_type": "text"
      },
      "source": [
        "**Tutorial from official PyTorch Website.**\n",
        "\n",
        "\n",
        "*pip install --upgrade git+https://github.com/pytorch/text for latest torchtext*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4StYqmcXZqV",
        "colab_type": "text"
      },
      "source": [
        "This tutorial shows how to train a supervised learning algorithm for classification using a Topic Classification Dataset - AG News\n",
        "\n",
        "Our goal is to classify a some document into 4 categories\n",
        "1. World\n",
        "2. Sport\n",
        "3. Business\n",
        "4. Sci / Tech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAwZzZhdKuMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade git+https://github.com/pytorch/text "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bv7u7RL8BDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBd4hSIcbYZY",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAV2rXMd8H2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "9e0da28d-e841-4296-b677-84985b16dbe9"
      },
      "source": [
        "NGRAMS = 2\n",
        "if not os.path.isdir('./data'):\n",
        "  os.mkdir('./.data')\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "BATCH_SIZE = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 16.2MB/s]\n",
            "120000lines [00:08, 13614.34lines/s]\n",
            "120000lines [00:17, 6695.84lines/s]\n",
            "7600lines [00:01, 6853.87lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyAkCUDyDyxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCxLgLSbfpF",
        "colab_type": "text"
      },
      "source": [
        "### Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5JOh92SA7hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrUDH6tjM44x",
        "colab_type": "text"
      },
      "source": [
        "### Initiate an Instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKFF3UP7BsDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUN_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Htwn73xM_O_",
        "colab_type": "text"
      },
      "source": [
        "### Function used to Generate Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubg6SC12Cf6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\n",
        "    # of elements in the dimension dim.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "\n",
        "    # concatenate all the texts\n",
        "    text = torch.cat(text)\n",
        "    return text, offsets, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tag4qViNIpS",
        "colab_type": "text"
      },
      "source": [
        "### Define Functions to Train the Model and Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-FDVDe2HVZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def train_func(sub_train_):\n",
        "  \n",
        "  # Train the model\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "  \n",
        "  for i, (text, offsets, cls) in enumerate(data):\n",
        "    optimizer.zero_grad()\n",
        "    text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "    output = model(text, offsets)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  # Adjust learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "\n",
        "def test(data_):\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  for text, offsets, cls in data:\n",
        "    text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(text, offsets)\n",
        "      loss = criterion(output, cls) \n",
        "      loss += loss.item()\n",
        "      acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PubOCEwxNUHk",
        "colab_type": "text"
      },
      "source": [
        "### Split the Dataset and Run the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfFAVAuuKv_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c107015a-ac26-413d-8a4b-6c254982856b"
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "N_EPOCHS = 10\n",
        "min_valid_loss = float(\"inf\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "train_len = int(len(train_dataset) * 0.95)\n",
        "\n",
        "sub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
        "\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train_func(sub_train_)\n",
        "  valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "  if valid_acc > best_valid_acc:\n",
        "    best_valid_acc = valid_acc\n",
        "  else:\n",
        "    break\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0262(train)\t|\tAcc: 84.8%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 90.1%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0119(train)\t|\tAcc: 93.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 90.3%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0069(train)\t|\tAcc: 96.4%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 90.9%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0038(train)\t|\tAcc: 98.1%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.0%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 99.0%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.1%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJx6XZlMNdfO",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the Model with Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNwz01JSMQ0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eaef9fd9-ea93-48d5-927b-29941ed7b58e"
      },
      "source": [
        "print('Checking the results of the test dataset...')\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of the test dataset...\n",
            "\tLoss: 0.0002(test)\t|\tAcc: 91.4%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8zJ8Q85P2pq",
        "colab_type": "text"
      },
      "source": [
        "### Test on Random News"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0brRCclfNKk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWHtv25-QHcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0b39360-5af5-489e-ed30-5331edb40d9c"
      },
      "source": [
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tech\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "  tokenizer = get_tokenizer(\"basic_english\")\n",
        "  with torch.no_grad():\n",
        "    text = torch.tensor([vocab[token] for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "    output = model(text, torch.tensor([0]))\n",
        "    return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t6aiHk6UfP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex_business = \"The S&P 500 and the Dow industrials slipped in a shortened, pre-Christmas session on Tuesday, as investors paused after a record-setting rally fueled by improving U.S.-China trade relations that has put the market on course for its best year since 2013.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l0JiIjkU30e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c749c6b1-886a-4144-c71e-5ca9cf8438e3"
      },
      "source": [
        "print(\"This is a %s news\" %ag_news_label[predict(ex_business, model, vocab, 2)])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Business news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqWcLQmfU5rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex_sci = '''\n",
        "We know a lot about Lego bricks, especially how they interact with bare feet. A team of physicists at Lancaster University in the UK wanted to learn something new, so they chilled some Lego pieces to near absolute zero and discovered they have some intriguing thermal properties.\n",
        "The scientists used a special \"dilution refrigerator\" that plunged the plastic to a mere 4 millidegrees above absolute zero. Absolute zero is theoretically the lowest temperature possible, which works out to  –459.67 degrees Fahrenheit or –273.15 degrees Celsius. \n",
        "The researchers popped four stacked bricks and an astronaut minifigure into the super-cool machine. You can check out the complicated process in a video deep-dive into the experiment.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoDwUuOSVQ13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e1c4848-dad8-43ef-eab3-5b97eb32d3ac"
      },
      "source": [
        "print(\"This is a %s news\" %ag_news_label[predict(ex_sci, model, vocab, 2)])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sci/Tec news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa2nkZAvVS91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex_world = '''\n",
        "About 2.4 billion people -- about half the population of Asia -- live in areas vulnerable to extreme weather events.\n",
        "This year, flooding and landslides, triggered by torrential monsoon rains, swept across India, Nepal, Pakistan, and Bangladesh, leaving devastation in each country and hundreds of deaths.\n",
        "China, Vietnam, Japan, India, Bangladesh, South Korea, Thailand, Sri Lanka and the Philippines, were all hit by tropical storms and typhoons -- or cyclones -- in 2019, causing dozens of deaths, hundreds of thousands displaced and millions of dollars in damage.\n",
        "The climate crisis is expected to create higher storm surges, increased rainfall and stronger winds.\n",
        "Joanna Sustento has been campaigning for climate action since Typhoon Haiyan devastated her home in Tacloban, the Philippines, in 2013.\n",
        "Sustento lost both her parents, her eldest brother, sister-in-law, and her young nephew in the storm -- one of the most powerful ever recorded.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrpjjyUpY-67",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9484507-49db-4195-86f6-d958b3b7a2db"
      },
      "source": [
        "print(\"This is a %s news\" %ag_news_label[predict(ex_world, model, vocab, 2)])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a World news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft2ODn3mZAqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex_scifi = '''\n",
        "Intelligent machines catastrophically misinterpreting human desires is a frequent trope in science fiction, perhaps used most memorably in Isaac Asimov’s stories of robots that misconstrue the famous “three laws of robotics.” The idea of artificial intelligence going awry resonates with human fears about technology. But current discussions of superhuman A.I. are plagued by flawed intuitions about the nature of intelligence.\n",
        "We don’t need to go back all the way to Isaac Asimov — there are plenty of recent examples of this kind of fear. Take a recent Op-Ed essay in The New York Times and a new book, “Human Compatible,” by the computer scientist Stuart Russell. Dr. Russell believes that if we’re not careful in how we design artificial intelligence, we risk creating “superintelligent” machines whose objectives are not adequately aligned with our own.\n",
        "As one example of a misaligned objective, Dr. Russell asks, “What if a superintelligent climate control system, given the job of restoring carbon dioxide concentrations to preindustrial levels, believes the solution is to reduce the human population to zero?” He claims that “if we insert the wrong objective into the machine and it is more intelligent than us, we lose.”\n",
        "Dr. Russell’s view expands on arguments of the philosopher Nick Bostrom, who defined A.I. superintelligence as “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.” Dr. Bostrom and Dr. Russell envision a superintelligence with vast general abilities unlike today’s best machines, which remain far below the level of humans in all but relatively narrow domains (such as playing chess or Go).\n",
        "Dr. Bostrom, Dr. Russell and other writers argue that even if there is just a small probability that such superintelligent machines will emerge in the foreseeable future, it would be an event of such magnitude and potential danger that we should start preparing for it now. In Dr. Bostrom’s view, “a plausible default outcome of the creation of machine superintelligence is existential catastrophe.” That is, humans would be toast.\n",
        "These thinkers — let’s call them the “superintelligentsia” — speculate that if machines were to attain general human intelligence, the machines would quickly become superintelligent. They speculate that a computer with general intelligence would be able to speedily read all existing books and documents, absorbing the totality of human knowledge. Likewise, the machine would be able to use its logical abilities to make discoveries that increase its cognitive power.\n",
        "Such a machine, the speculation goes, would not be bounded by bothersome human limitations, such as slowness of thought, emotions, irrational biases and need for sleep. Instead, the machine would possess something like a “pure” intelligence without any of the cognitive shortcomings that limit humans.\n",
        "The assumption seems to be that this A.I. could surpass the generality and flexibility of human intelligence while seamlessly retaining the speed, precision and programmability of a computer. This imagined machine would be far smarter than any human, far better at “general wisdom and social skills,” but at the same time it would preserve unfettered access to all of its mechanical capabilities. And as Dr. Russell’s example shows, it would lack humanlike common sense.\n",
        "The problem with such forecasts is that they underestimate the complexity of general, human-level intelligence. Human intelligence is a strongly integrated system, one whose many attributes — including emotions, desires, and a strong sense of selfhood and autonomy — can’t easily be separated.\n",
        "Similarly, if generally intelligent A.I. is ever created (something that will take many decades, if not centuries), its objectives, like ours, will not be easily “inserted” or “aligned.” They will rather develop along with the other qualities that form its intelligence, as a result of being embedded in human society and culture. The machines’ push to achieve these objectives will be tempered by the common sense, values and social judgment without which general intelligence cannot exist.\n",
        "What’s more, the notion of superintelligence without humanlike limitations may be a myth. It seems likely to me that many of the supposed deficiencies of human cognition are inseparable aspects of our general intelligence, which evolved in large part to allow us to function as a social group. It’s possible that the emotions, “irrational” biases and other qualities sometimes considered cognitive shortcomings are what enable us to be generally intelligent social beings rather than narrow savants. I can’t prove it, but I believe that general intelligence can’t be isolated from all these apparent shortcomings, either in humans or in machines that operate in our human world.\n",
        "In his 1979 Pulitzer Prize-winning book, “Gödel, Escher, Bach: an Eternal Golden Braid,” the cognitive scientist Douglas Hofstadter beautifully captures the counterintuitive complexity of intelligence by posing a deceptively simple question: “Will a thinking computer be able to add fast?” Dr. Hofstadter’s surprising but insightful answer was, “perhaps not.”\n",
        "As Dr. Hofstadter explains: “We ourselves are composed of hardware which does fancy calculations but that doesn’t mean that our symbol level, where ‘we’ are, knows how to carry out the same fancy calculations. Let me put it this way: There’s no way that you can load numbers into your own neurons to add up your grocery bill. Luckily for you, your symbol level (i.e., you) can’t gain access to the neurons which are doing your thinking — otherwise you’d get addlebrained.” So, why, he asks, “should it not be the same for an intelligent program?”\n",
        "In other words, the intelligent part of your mind can’t harness the fast-adding skills of your own neurons, and for good reason. This barrier — between the “self” that you are aware of and the detailed activity of your brain — permits the kind of thinking that matters for survival without getting overwhelmed (“addlebrained”) by your own thought processes. Similarly, a thinking computer’s hardware, like ours, would presumably include circuits for fast arithmetic, but at the level of its cognitive awareness, the machine wouldn’t be able to tap into these circuits any more than we humans can.\n",
        "It’s fine to speculate about aligning an imagined superintelligent — yet strangely mechanical — A.I. with human objectives. But without more insight into the complex nature of intelligence, such speculations will remain in the realm of science fiction and cannot serve as a basis for A.I policy in the real world.\n",
        "Understanding our own thinking is a hard problem for our plain old intelligent minds. But I’m hopeful that we, and our future thinking computers, will eventually achieve such understanding in spite of — or perhaps thanks to — our shared lack of superintelligence.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVSCISkazOS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20ad463e-b35d-4e23-e6e1-25edbc09efa3"
      },
      "source": [
        "print(\"This is a %s news\" %ag_news_label[predict(ex_scifi, model, vocab, 2)])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sci/Tec news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMzx-6Pa1q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}